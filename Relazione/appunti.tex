\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

% Questo comando importa i miei comandi personali che semplificano due o tre cagate
\input{.Latex_stuff/commands.tex}
%\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{arydshln}
\usepackage{subcaption}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{tikz}
\newcommand{\lira}{%
\begin{tikzpicture}[x=0.08em, y=0.08em, xscale=0.03, yscale=-0.03, inner sep=0pt, outer sep=0pt]
\fill (54.3355,9.3092) .. controls (70.3869,9.3092) and (79.7110,9.3092) ..
  (82.3075,9.3092) .. controls (82.3075,12.1418) and (82.3075,24.2985) ..
  (82.3075,45.7791) .. controls (82.3075,67.2598) and (82.3075,79.2984) ..
  (82.3075,81.8950) -- (167.2860,51.0903) .. controls (167.2860,59.3521) and
  (167.2860,66.7877) .. (167.2860,73.3971) -- (82.3075,104.2018) .. controls
  (82.3075,105.1460) and (82.3075,107.1525) .. (82.3075,110.2211) .. controls
  (82.3075,113.2898) and (82.3075,115.2962) .. (82.3075,116.2404) --
  (128.3375,99.9529) -- (167.2860,85.4358) .. controls (167.2860,86.8521) and
  (167.2860,90.4518) .. (167.2860,96.2351) .. controls (167.2860,102.0184) and
  (167.2860,105.5001) .. (167.2860,106.6804) .. controls (167.2860,107.6246) and
  (167.0499,108.0967) .. (166.5778,108.0967) -- (84.7861,137.4851) .. controls
  (83.8419,137.9572) and (83.0157,138.4293) .. (82.3075,138.9014) .. controls
  (82.3075,153.3005) and (82.3075,173.1878) .. (82.3075,198.5633) .. controls
  (82.3075,223.9388) and (82.3075,243.8262) .. (82.3075,258.2253) .. controls
  (82.3075,258.2253) and (82.3075,258.3433) .. (82.3075,258.5794) .. controls
  (82.3075,258.8154) and (82.3075,258.9334) .. (82.3075,258.9334) .. controls
  (103.7882,256.3369) and (122.7903,248.1931) .. (139.3139,234.5021) .. controls
  (157.4899,219.6309) and (169.6465,201.1009) .. (175.7838,178.9121) .. controls
  (178.3804,168.7619) and (179.6787,158.6117) .. (179.6787,148.4614) .. controls
  (179.6787,148.4614) and (189.1207,148.4614) .. (208.0048,148.4614) .. controls
  (208.0048,171.3584) and (202.4576,192.9571) .. (191.3632,213.2575) .. controls
  (183.5735,228.8369) and (172.9512,242.2918) .. (159.4963,253.6223) .. controls
  (146.9856,264.7167) and (132.9405,273.2145) .. (117.3611,279.1158) .. controls
  (97.0607,286.9055) and (76.0522,289.6201) .. (54.3355,287.2596) .. controls
  (54.3355,271.9163) and (54.3355,248.9013) .. (54.3355,218.2146) .. controls
  (54.3355,187.5279) and (54.3355,164.3949) .. (54.3355,148.8155) .. controls
  (54.3355,148.8155) and (52.8011,149.4057) .. (49.7325,150.5859) --
  (1.2239,167.9357) .. controls (1.2239,160.8541) and (1.2239,153.4185) ..
  (1.2239,145.6288) -- (54.3355,126.5087) .. controls (54.3355,125.0924) and
  (54.3355,120.9615) .. (54.3355,114.1160) .. controls (51.7389,115.2962) and
  (48.2571,116.7126) .. (43.8902,118.3649) .. controls (39.5232,120.0173) and
  (36.7496,120.9615) .. (35.5694,121.1975) -- (7.5973,131.4658) .. controls
  (7.5973,131.4658) and (6.8301,131.7018) .. (5.2958,132.1739) .. controls
  (3.7615,132.6460) and (2.4042,133.0001) .. (1.2239,133.2361) .. controls
  (1.2239,121.9057) and (1.2239,114.4701) .. (1.2239,110.9293) --
  (54.3355,92.1632) .. controls (54.3355,81.7770) and (54.3355,67.9680) ..
  (54.3355,50.7362) .. controls (54.3355,33.5045) and (54.3355,19.6955) ..
  (54.3355,9.3092) -- cycle;

\end{tikzpicture}}


\sloppy
\begin{document}

\begin{titlepage}
\begin{center}
	\Large{\textbf{Algorithms for Massive Datasets Project:\\Turkish lira recognizer}}
\vfill
\normalsize{Caccaro Sebastiano, Mat. 958683}\\
\normalsize{Cavagnino Matteo, Mat. 961707}\\
\normalsize{A.A.2019/2020}
\end{center}
\end{titlepage}
\pagenumbering{Roman}

\vspace*{\fill}
\textit{We declare that this material, which We now submit for assessment, is entirely our own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of our work. We understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should we engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by us or any other person for assessment on this or any other course of study.}
\vspace*{\fill}

\newpage

\tableofcontents

\clearpage
%magari c'Ã¨ qualcosa di un po piu slick di una section per questo

\pagenumbering{arabic}

\newpage
\section{Introduction}

The goal of this project is to build and train a classifier for Turkish Liras based on the \textit{Turkish Lira Banknote Dataset} (\autoref{sec:dataset}).\\
The classifier is developed using Convolutional Neural Networks (from now on CNN). The CNN is coded using python3 and using the Tensorflow and Keras libraries.

\subsection{References and sources}
In order to develop the project some other sources other than the ones given during the course were consulted: all the links are cited in the \textit{Reference} section of this document.\\
The sources include also some links from the \textit{Tensorflow official documentation} from which most of the code in the project has been adapted.

\subsection{Links}
All of the code from the project is available at the following link:
\begin{center}
\url{https://github.com/sebacaccaro/Progetto_AFMD}
\end{center}
The code consists of a notebook which is meant to be run on Google Colab. In order to execute it, a Kaggle API in JSON format is needed






\newpage
\section{Dataset}
\label{sec:dataset}
The dataset used for the project is the \textit{Turkish Lira Banknote Dataset}, available at the following link:
\begin{center}
\url{https://www.kaggle.com/baltacifatih/turkish-lira-banknote-dataset}
\end{center}

The dataset is composed by 6000 images of Turkish Lira Banknotes, organized as follows:
\begin{itemize}
\item 1000 pictures of 5\lira\ banknotes
\item 1000 pictures of 10\lira\ banknotes
\item 1000 pictures of 20\lira\ banknotes
\item 1000 pictures of 50\lira\ banknotes
\item 1000 pictures of 100\lira\ banknotes
\item 1000 pictures of 200\lira\ banknotes
\end{itemize}

Images are already splitted in \textit{train} and \textit{validation}. The training set contains 925 images for each banknote, leading to 92.5\% of the images (5550) begin used for training and the remaining 7.5\% (450) being used for validation.

\subsection{Images}
All the pictures in the dataset consist of picture of banknotes taken with different perspectives and conditions: some are just laying on different surfaces, some are held in hand, blurred, folded or partially out of frame, ecc.

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Immagini/folded}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Immagini/plain}
  \label{fig:sub2}
\end{subfigure}
\caption{Example of images of 20\lira\ banknotes }
\label{fig:test}
\end{figure}

The author of the dataset already performed brightness adjustment, noise reduction and image flipping when necessary.\\
All images are in PNG format with a 1280 x 720 resolution.

\subsection{Preprocessing}
Taking into consideration what has already been said about the images, it is clear that there is no need to apply other forms of corrections. Therefore, the only steps necessary prior to the train phase are:
\begin{itemize}
	\item Image scaling and decoding
	\item Dataset loading and batch splitting
\end{itemize}

\subsubsection{Image Scaling}
All the images come in a 1280x720 resolution. Working with such high resolution it's not 
recommended when training a CNN as it would take a big tall on the training speed due to the increasing number of parameters and RAM usage. 
For this reason all the images are reduced by a scale factor of 5, adjusting their resolution to 140x256.\\
Every image is also decoded in a 3D tensor with 3 channels corresponding to its RGB values.

\subsubsection{Dataset loading and batch splitting}
For both the training and the validation datasets a list is created containing the path of their images. 
Every element in the lists is then mapped to an image-label pair. The lists are then used to create the actual dataset variables.\\
Each dataset is divided into batches. After some tweaking, a number of 32 elements per batch proved to be optimal for the considered application.\\
Each dataset is set to repeat, allowing for multiple epochs of training on the same batches. Moreover, the training dataset is set to be prefetched: 
this allows the next batches to be loaded while the current one is being used, drastically reducing latency and improving throughput at the cost using additional memory.\\
Only the training dataset is shuffled as it is not really necessary with the validation dataset, since it is only used for evaluation.

\subsubsection{Caching}
During testing, the dataset has been set to be cached in RAM to significantly reduce training time. The dataset used in the project occupies about 3GB, and with image resizing it comfortably fits in memory. This may not be true for larger datasets. Just trying to cache the project dataset without images resizing causes the memory to get filled up and crash the Google Colab instance, as the dimensions inflates when converting images to tensors.\\
Because of this, we can safely assume caching would not work for larger datasets.



\newpage
\section{Experiments and results}
Different models have been tested during the developing process; in this section some of those will be shown and the relative results will be discussed.
\subsection{Model summary}
For each tested model the following data will be reported:
\begin{itemize}
\item The NN architecture
\item Hyperparameters used
\item Data on accuracy for three repeated runs
\item Graph of one of the runs
\item Comments on the architecture and results
\end{itemize}
In the layer tables the input  layer will not be reported, as it always corresponds to resized image size (\texttt{144,256,3}).\\
Also note that some abbreviations are used in the Layer Config field in order for the table to fit:
\begin{itemize}
\item \texttt{k} stands for \texttt{kernel size}
\item \texttt{s} stands for \texttt{strides}
\item \texttt{f} stands for \texttt{filters}
\item \texttt{p} stands for \texttt{pool size}
\item \texttt{r} stands for \texttt{rate}
\end{itemize}

%Commands needed for table
\newcommand{\conv}{Convolution(\texttt{Conv2d})}
\newcommand{\convP}[3]{\texttt{k=#1, s=#2, f=#3}}
\newcommand{\convKSF}[3]{\convP{#1}{#2}{#3}}

\newcommand{\flt}{Flatten(\texttt{Flatten})}

\newcommand{\dns}{Dense(\texttt{Dense})}
\newcommand{\dnsP}[1]{\texttt{u=#1}}

\newcommand{\pool}{MaxPooling(\texttt{MaxPooling2D})}
\newcommand{\poolN}{\texttt{p=2x2}}

\newcommand{\drop}{Dropout(\texttt{Dropout})}
\newcommand{\dropR}[1]{\texttt{r=#1}}

\newcommand{\bat}{Batch Norm.(\texttt{BatchN.})}

\newpage

\subsection{Models}

\subsubsection{Baseline Model}
\input{Models/baseline.tex}

\subsubsection{Convolution Model}
\input{Models/conv.tex}

\subsubsection{Convolution and Pooling Model}
\input{Models/conv_pool.tex}

\subsubsection{Convolution and Pooling Model with Dropout}
\input{Models/conv_pool_drop.tex}

\subsubsection{Batch Normalization Model}
\input{Models/bn.tex}

\subsection{Models comparison}
\input{Models/comparison.tex}




\newpage
\section{Scalability}
So far, the dataset has been fully cached in RAM during training. This is a viable solution when the dataset is quite small, but it couldn't work with bigger datasets.\\
The goal is to be able to perform training on larger datasets, without losing too much in training speed. In order to assess training speed the following procedure id used:
\begin{itemize}
\item All the tried solutions are measured on the first model.
\item During training, the timestamp of every epoch end is saved on a list.
\item After training, the average time between epochs end is calculated. This is the measure used to assess training speed.
\end{itemize}

\subsection{No caching}
The first step is not to cache anything and run the model strait away. This solution would possibly work for any dataset, but it is tremendously slow (even with prefetch enabled).
As a result, it is not recommend to use this solution.

\subsection{File caching}
This technique uses a file to cache the dataset. This greatly increases performance, but the cache file can take up some space. Moreover, the first epoch of training will take some time to execute, as the cache file need to be exectu

\subsection{TFRecords}
As seen in the previous solutions, the main bottleneck in training is in fetching the data from the storage. In order to speed up this phase, it is possible to serialize the dataset in order to be able to access data much quicker. This process is showcased in the second notebook.\\
 Training speed performance at this point is notably improved, but there is a significant slowdown at the beginning of every epoch. In order to reduce it, images are resized before creating the TFRecord. This has two advantages:
\begin{itemize}
\item The TFRecord file itself is smaller, and therefore faster to read and process
\item Images do not need to be resized multiple times during the training process
\end{itemize}
 
\subsection{Results}
\begin{table}
	\centering
	\begin{tabular}{ccc}
	\textbf{Technique} & \textbf{Seconds per Epoch} & \textbf{Disk space*}\\ \hline
	Memory Caching 				& 3.1216 & /\\ \hdashline
	No Caching					& 117.6890 & / \\
	TFRecords 					& 8.8295 & \ 279.51 Mb (07.58\%)\\
	File Caching					& 7.0811	 & 2534.45 Mb (68.75\%)\\
	File Caching with pre-resizing & 3.8539 & 2534.45 Mb (68.75\%)\\
	File Caching and TFRecords 	& 4.0606 &2733.10 Mb (74,14\%)\\
	\end{tabular}
	%TOTAL = 3686.4
	\captionsetup{labelformat=empty}
	\caption{\textit{*Disk space is shown both as a absolute value and as percentage of the original uncompressed dataset}}
\end{table}

Results show it is possible to have comparable training speed with larger datasets if some time is invested upfront in serializing or caching the dataset.\\
After analyzing the results, the following considerations can be made:
\begin{itemize}
\item File caching with pre-resizing is the fastest solution, but it requires a lot of disk space, which could be a problem when dealing with big datasets. If such space is available it is the preferred solution.
\item TFRecords seem the most viable solution. Although it's marginally slower solution than File Caching (2x slowdown), it's much more convenient in terms of disk space:
\begin{itemize}
\item The required space is one order of magnitude smaller than File Caching
\item When creating a TFRecord it is possible, as it is performed in the notebook, to delete a raw image pair after writing it on the record. This means that, as long as the dataset fits on disk, this a viable solution. It would also be possible to perform the serialization on a more powerful machine (or distributed system) and perform the training on a platform where the original dataset may not have fitted in a raw format.
\end{itemize}
\end{itemize}



\newpage
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}
\bibitem{kaggle-colab} 
Easiest way to download kaggle data in Google Colab\\
\url{https://www.kaggle.com/general/74235}

\bibitem{images} 
Load images\\
\url{https://www.tensorflow.org/tutorials/load_data/images}

\bibitem{images} 
Classification\\
\url{https://www.tensorflow.org/tutorials/images/classification}

\bibitem{CNN}
Convolutional Neural Network (CNN)\\
\url{https://www.tensorflow.org/tutorials/images/cnn} 

\bibitem{CNNGuide}
A Comprehensive Guide to Convolutional Neural Networks â the ELI5 way\\
\url{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-\\neural-networks-the-eli5-way-3bd2b1164a53} 

\bibitem{CNNGuide2}
A guide to an efficient way to build neural network architectures- Part II: Hyper-parameter selection and tuning for Convolutional Neural Networks using Hyperas on Fashion-MNIST\\
\url{https://towardsdatascience.com/a-guide-to-an-efficient-way\\-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d73} 

\bibitem{CNNGuide2}
TFRecords\\
\url{https://www.tensorflow.org/tutorials/load_data/tfrecord}

\bibitem{notebook}
Prof. Notebook on deep learning\\







\end{thebibliography}


\end{document}
