{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progetto AFMD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZdFL2gq5s05",
        "colab_type": "text"
      },
      "source": [
        "# Progetto AFMD\n",
        "Caccaro Sebastiano - Cavagnino Matteo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKaUS4s8rkl",
        "colab_type": "text"
      },
      "source": [
        "## Database import\n",
        "\n",
        "### Download and extraction\n",
        "The database is imported via the kaggle API token. They API key is needed in order to obtain access to kaggle. In the final py file, the upload part must be scrapped.\n",
        "\n",
        "https://www.kaggle.com/general/74235\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyFkgAVJ9VbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Please upload your kaggle API Key\")\n",
        "\n",
        "keyname = \"kaggle.json\"\n",
        "!rm $keyname\n",
        "file = files.upload()\n",
        "filename = list(file.keys())[0]\n",
        "! mkdir ~/.kaggle\n",
        "! mv $filename ~/.kaggle/$keyname\n",
        "! chmod 600 ~/.kaggle/$keyname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHEWK2OVBIsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset download. It may take a while\n",
        "!kaggle datasets download -d baltacifatih/turkish-lira-banknote-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcDoq_JvDS6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "#Unzip of the database. It may take a while\n",
        "!mkdir dataset\n",
        "!unzip turkish-lira-banknote-dataset.zip -d dataset\n",
        "\n",
        "#Output is suppressed because it's very long"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1LJWWhvPmiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24an2BaSF8hR",
        "colab_type": "text"
      },
      "source": [
        "### Images import\n",
        "Importing the images with tensorflow. All the the reference is taken from the following links\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szfc5JbmGd0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "from IPython.display import clear_output, display\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFalG_3JHFH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = pathlib.Path(\"/content/dataset\")\n",
        "data_dir.lstat()\n",
        "image_count = len(list(data_dir.glob('*/*.png')))\n",
        "print (\"Number of dataset images: \" + str(image_count))\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name not in [\"train.txt\",\"validation.txt\"]])\n",
        "print (\"Labels :\" + str(CLASS_NAMES))\n",
        "\n",
        "fivers = list(data_dir.glob('5/*'))\n",
        "\n",
        "print('Example image:\\n')\n",
        "for image_path in fivers[:1]:\n",
        "    display(Image.open(str(image_path)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpbIS5LCKvD8",
        "colab_type": "text"
      },
      "source": [
        "### Images Loading\n",
        "The following set of funtions is needed to better load the images into the database. Note that `decode img` also handles **image resizing**. <br>\n",
        "It is important to set the right `IMG_HEIGHT` and `IMG_WIDTH`. Note also thath, since all image are `1280x720`, we'll be resizing them by a constant factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4suh3qseKn__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "# DO NOT MODIFY\n",
        "ORIGINAL_HEIGHT = 720\n",
        "ORIGINAL_WIDTH = 1280\n",
        "\n",
        "#DO MODIFY\n",
        "SCALE_FACTOR = 5\n",
        "\n",
        "IMG_HEIGHT = math.floor(ORIGINAL_HEIGHT/SCALE_FACTOR)\n",
        "IMG_WIDTH = math.floor(ORIGINAL_WIDTH/SCALE_FACTOR)\n",
        "\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  return parts[-2] == CLASS_NAMES\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_png(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "  return img #tf.reshape(img, [IMG_HEIGHT*IMG_WIDTH])    \n",
        "\n",
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w5ZHuwgKJ6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split dataset (in training and validation datasets)\n",
        "training_ds = np.array(['./dataset/'+ item.rstrip() for item in open(\"./dataset/train.txt\", 'r')])\n",
        "validation_ds = np.array(['./dataset/' + item.rstrip() for item in open(\"./dataset/validation.txt\", 'r')])\n",
        "num_train = len(training_ds)\n",
        "num_val = len(validation_ds)\n",
        "\n",
        "\n",
        "#Creating and loading the datasets\n",
        "list_training_ds = tf.data.Dataset.list_files(training_ds)\n",
        "labeled_training_ds = list_training_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "list_validation_ds = tf.data.Dataset.list_files(validation_ds)\n",
        "labeled_validation_ds = list_validation_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "print(\"Printing info for the first image. Labels are expressed in one-hot encoding\")\n",
        "\n",
        "print(\"\\nTraining set example:\")\n",
        "for image, label in labeled_training_ds.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())\n",
        "\n",
        "print(\"\\nValidation set example:\")\n",
        "for image, label in labeled_validation_ds.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a592T_2dUzga",
        "colab_type": "text"
      },
      "source": [
        "The following snipped shuffles and divide into batches the original database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky8Wi0AGVH6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(6,6,n+1)\n",
        "      plt.imshow(image_batch[n])\n",
        "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
        "      plt.axis('off')\n",
        "\n",
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
        "  # This is a small dataset, only load it once, and keep it in memory.\n",
        "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "  # fit in memory.\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.batch(BATCH_SIZE,drop_remainder=True)\n",
        "\n",
        "  ds = ds.repeat()\n",
        "\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return ds\n",
        "\n",
        "def prepare_for_validation(dataset,cache=True):\n",
        "    if cache:\n",
        "      if isinstance(cache, str):\n",
        "        dataset = dataset.cache(cache)\n",
        "      else:\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "\n",
        "    #all test elements in one batch\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.repeat()\n",
        "    return dataset\n",
        "\n",
        "#create a prefetch dataset for the training dataset\n",
        "train_ds = prepare_for_training(labeled_training_ds)\n",
        "\n",
        "#create a repeat dataset for the validation\n",
        "validation_ds = prepare_for_validation(labeled_validation_ds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGcBJzbMd-Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0dkyLk_4VVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#try to pull one train batch and show the result\n",
        "train_image_batch, train_label_batch = next(iter(train_ds))\n",
        "print(\"First 25 images of the first training batch\")\n",
        "show_batch(train_image_batch.numpy(), train_label_batch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiLZQRcasTnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlotTraining(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, size, sample_rate=1, zoom=1):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.step = 0\n",
        "    self.zoom = zoom\n",
        "    self.steps_per_epoch = size//BATCH_SIZE\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.batch_history = {}\n",
        "    self.batch_step = []\n",
        "    self.epoch_history = {}\n",
        "    self.epoch_step = []\n",
        "    self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    plt.ioff()\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    if (batch % self.sample_rate) == 0:\n",
        "      self.batch_step.append(self.step)\n",
        "      for k,v in logs.items():\n",
        "        # do not log \"batch\" and \"size\" metrics that do not change\n",
        "        # do not log training accuracy \"acc\"\n",
        "        if k=='batch' or k=='size':# or k=='acc':\n",
        "          continue\n",
        "        self.batch_history.setdefault(k, []).append(v)\n",
        "    self.step += 1\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    plt.close(self.fig)\n",
        "    self.axes[0].cla()\n",
        "    self.axes[1].cla()\n",
        "      \n",
        "    self.axes[0].set_ylim(0, 1.2/self.zoom)\n",
        "    self.axes[1].set_ylim(1-1/self.zoom/2, 1+0.1/self.zoom/2)\n",
        "    \n",
        "    self.epoch_step.append(self.step)\n",
        "    for k,v in logs.items():\n",
        "      # only log validation metrics\n",
        "      if not k.startswith('val_'):\n",
        "        continue\n",
        "      self.epoch_history.setdefault(k, []).append(v)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    for k,v in self.batch_history.items():\n",
        "      (self.axes[0 if k.endswith('loss') else 1]\n",
        "           .plot(np.array(self.batch_step) / self.steps_per_epoch, v, label=k))\n",
        "      \n",
        "    for k,v in self.epoch_history.items():\n",
        "      (self.axes[0 if k.endswith('loss') else 1]\n",
        "           .plot(np.array(self.epoch_step) / self.steps_per_epoch, v,\n",
        "                 label=k, linewidth=3))\n",
        "      \n",
        "    self.axes[0].legend()\n",
        "    self.axes[1].legend()\n",
        "    self.axes[0].set_xlabel('epochs')\n",
        "    self.axes[1].set_xlabel('epochs')\n",
        "    self.axes[0].minorticks_on()\n",
        "    self.axes[0].grid(True, which='major', axis='both',\n",
        "                      linestyle='-', linewidth=1)\n",
        "    self.axes[0].grid(True, which='minor', axis='both',\n",
        "                      linestyle=':', linewidth=0.5)\n",
        "    self.axes[1].minorticks_on()\n",
        "    self.axes[1].grid(True, which='major', axis='both',\n",
        "                      linestyle='-', linewidth=1)\n",
        "    self.axes[1].grid(True, which='minor', axis='both',\n",
        "                      linestyle=':', linewidth=0.5)\n",
        "    display(self.fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO9fCNzyVkS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_1():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=5, kernel_size=5, strides=3, activation='relu', input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(len(CLASS_NAMES),activation=\"softmax\"))\n",
        "  model.summary()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        " \n",
        "  plot_training = PlotTraining(5550, sample_rate=10, zoom=5)\n",
        "  history = None\n",
        "\n",
        "  train_steps = int(num_train/BATCH_SIZE)\n",
        "  validation_steps = int(num_val /BATCH_SIZE)\n",
        "\n",
        "  EPOCHS = 20\n",
        "\n",
        "  history = model.fit(x=train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      validation_steps=validation_steps,\n",
        "                      epochs=EPOCHS, \n",
        "                      steps_per_epoch=train_steps,\n",
        "                      callbacks=[plot_training]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wgHNHbKVkdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_2():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=5, kernel_size=5, strides=3, activation='relu', input_shape=(IMG_HEIGHT,IMG_WIDTH, 3), padding='same'))\n",
        "  model.add(layers.Conv2D(filters=8, kernel_size=5, strides=2, activation='relu', padding='same'))\n",
        "  model.add(layers.Conv2D(filters=12, kernel_size=3, strides=1, activation='relu'))\n",
        "  model.add(layers.Conv2D(filters=15, kernel_size=3, strides=1, activation='relu'))\n",
        "  model.add(layers.Conv2D(filters=18, kernel_size=3, strides=1, activation='relu')) \n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(len(CLASS_NAMES),activation=\"softmax\")) \n",
        "  model.summary()\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  plot_training = PlotTraining(5550, sample_rate=10, zoom=5)\n",
        "  history = None\n",
        "\n",
        "  train_steps = int(num_train/BATCH_SIZE)\n",
        "  validation_steps = int(num_val /BATCH_SIZE)\n",
        "\n",
        "  EPOCHS = 20\n",
        "\n",
        "  history = model.fit(x=train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      validation_steps=validation_steps,\n",
        "                      epochs=EPOCHS, \n",
        "                      steps_per_epoch=train_steps,\n",
        "                      callbacks=[plot_training]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czF1hOz3YnXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_3():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=5, kernel_size=5, strides=1, activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=8, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=12, kernel_size=3, strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=15, kernel_size=3, strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=18, kernel_size=3,strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "  model.summary()\n",
        "  plot_training = PlotTraining(5550, sample_rate=10, zoom=5)\n",
        "  history = None\n",
        " \n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  train_steps = int(num_train/BATCH_SIZE)\n",
        "  validation_steps = int(num_val /BATCH_SIZE)\n",
        "\n",
        "  EPOCHS = 20\n",
        "\n",
        "  history = model.fit(x=train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      validation_steps=validation_steps,\n",
        "                      epochs=EPOCHS, \n",
        "                      steps_per_epoch=train_steps,\n",
        "                      callbacks=[plot_training]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4BZ3V0MVklE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_4():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=5, kernel_size=5, strides=1, activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=8, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=12, kernel_size=3,strides=1, activation='relu',  padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=15, kernel_size=3, strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.Conv2D(filters=18, kernel_size=3,strides=1, activation='relu', padding='same'))\n",
        "  model.add(layers.Dropout(.075))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dropout(.075))\n",
        "  model.add(layers.Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "  model.summary()\n",
        " \n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  plot_training = PlotTraining(5550, sample_rate=10, zoom=5)\n",
        "  history = None\n",
        "\n",
        "  train_steps = int(num_train/BATCH_SIZE)\n",
        "  validation_steps = int(num_val /BATCH_SIZE)\n",
        "\n",
        "  EPOCHS = 20\n",
        "\n",
        "  history = model.fit(x=train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      validation_steps=validation_steps,\n",
        "                      epochs=EPOCHS, \n",
        "                      steps_per_epoch=train_steps,\n",
        "                      callbacks=[plot_training]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gRiWegRGd0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def model_5():  \n",
        "  plot_training = PlotTraining(5550, sample_rate=10, zoom=5)\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=5, kernel_size=5, strides=1, use_bias=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "\n",
        "  model.add(layers.Conv2D(filters=8, kernel_size=5, strides=1, use_bias=False, padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "\n",
        "  model.add(layers.Conv2D(filters=12, kernel_size=3, strides=1, use_bias=False,  padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "\n",
        "  model.add(layers.Conv2D(filters=15, kernel_size=3, strides=1, use_bias=False, padding='same'))\n",
        "  model.add(layers.MaxPooling2D())\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "  model.add(layers.Dropout(0.06))\n",
        "\n",
        "  model.add(layers.Conv2D(filters=18, kernel_size=3, strides=1, use_bias=False, padding='same'))\n",
        "  \n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "\n",
        "  model.add(layers.Dense(64,use_bias=False))\n",
        "  model.add(layers.BatchNormalization(center=True, scale=False))\n",
        "  tf.keras.layers.Activation('relu')\n",
        "  model.add(layers.Dropout(0.06))\n",
        "\n",
        "  model.add(layers.Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  history = None\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "  model.compile(optimizer=opt, # TODO: We should also tweak with the learing rate\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  train_steps = int(num_train/BATCH_SIZE)\n",
        "  validation_steps = int(num_val /BATCH_SIZE)\n",
        "\n",
        "  EPOCHS = 50           #TODO Probabilmente da aumentare\n",
        "\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "  history = model.fit(x=train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      validation_steps=validation_steps,\n",
        "                      epochs=EPOCHS, \n",
        "                      steps_per_epoch=train_steps,\n",
        "                      callbacks=[plot_training]#callbacks=[plot_training, lr_decay_callback]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQo_wO8ZGmB",
        "colab_type": "text"
      },
      "source": [
        "###Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHPWSptPZGB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhKVYDTZd_M",
        "colab_type": "text"
      },
      "source": [
        "###Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KzCljMJZR6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tkCwFlLZgps",
        "colab_type": "text"
      },
      "source": [
        "###Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E3VvSlRZSRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwF3BiGZh1B",
        "colab_type": "text"
      },
      "source": [
        "###Model 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFYwdACZSah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_4()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u11STkJdZjCZ",
        "colab_type": "text"
      },
      "source": [
        "###Model 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "063JG5GdZbXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_5()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}